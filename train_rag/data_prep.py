import jsonimport osimport loggingfrom datasets import load_dataset# Setting up logginglogging.basicConfig(level=logging.INFO)# Function to save a list of dictionaries as a JSONL filedef save_to_jsonl(path, items):    with open(path, "w", encoding="utf-8") as f:        for item in items:            f.write(json.dumps(item, ensure_ascii=False) + "\n")# Function to load a JSONL file into a list of dictionariesdef load_from_jsonl(path):    out = []    with open(path, "r", encoding="utf-8") as f:        for line in f:            out.append(json.loads(line))    return out# Safe getter functiondef g(d, *keys):    for k in keys:        if k in d and d[k] is not None:            return d[k]    return None# Initialize output directoryOUT_DIR = "prepared"os.makedirs(OUT_DIR, exist_ok=True)def normalize_all():    docs = []    idc = 0    dataset_names = {        "Beautiful-Chinese": ("Seikaijyu/Beautiful-Chinese", "question", "answer"),        "NCERT_Geography_12th": ("KadamParth/NCERT_Geography_12th", "text", "answer_text"),        "MATH-Algebra": ("themanas021/MATH-Algebra", "text", "answer"),        "history_v3": ("ambrosfitz/history_v3", "content", "summary"),    }    for name, (dataset, question_key, answer_key) in dataset_names.items():        try:            ds = load_dataset(dataset, split="train")            for ex in ds:                q = g(ex, question_key)                a = g(ex, answer_key)                if not q:                     continue                docs.append({"id": f"{name.lower()}_{idc}", "source": name.lower(), "text": q, "answer": a})                idc += 1            logging.info(f"Processed {name} dataset with {idc} entries.")        except Exception as e:            logging.error(f"Skipped {name}: {e}")    logging.info(f"Total documents collected: {len(docs)}")    save_to_jsonl(os.path.join(OUT_DIR, "docs.jsonl"), docs)    return docs# Chunk long docs (simple whitespace chunking)def chunk_docs(chunk_tokens=384, overlap=64):    docs = []    raw = load_from_jsonl(os.path.join(OUT_DIR, "docs.jsonl"))        for d in raw:        words = d["text"].split()        if len(words) <= chunk_tokens:            docs.append(d)            continue        for i in range(0, len(words), chunk_tokens - overlap):            chunk = " ".join(words[i:i + chunk_tokens])            docs.append({                "id": f"{d['id']}_c{i // (chunk_tokens - overlap)}",                "source": d["source"],                "text": chunk,                "answer": d.get("answer"),                "parent": d["id"],                "chunk_index": i // (chunk_tokens - overlap)            })        logging.info(f"Total chunks created: {len(docs)}")    save_to_jsonl(os.path.join(OUT_DIR, "chunked_docs.jsonl"), docs)    return docs# Example usageif __name__ == "__main__":    normalize_all()    chunk_docs()